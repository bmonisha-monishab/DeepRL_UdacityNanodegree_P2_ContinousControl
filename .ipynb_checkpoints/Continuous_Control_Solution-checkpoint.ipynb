{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Udacity Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) \n",
    "# Project 2: Continuous Control\n",
    "\n",
    "This notebook contains Nishi Soodâ€™s work for the Udacity's Deep Reinforcement Learning Nanodegree Project 2: Continuous Control. For this project, I have trained an agent that could control a double-jointed arm towards specific locations.\n",
    "\n",
    "The aim is to train an agent that is capable of dealing with continuous valued inputs and outputs. \n",
    "Deep Deterministic Policy Gradient algorithm([https://arxiv.org/pdf/1509.02971.pdf] is used for Solutioning\n",
    "\n",
    "## Project's goal\n",
    " - In this environment, a double-jointed arm can move to target locations. \n",
    " - A reward of +0.1 is provided for each step that the agent's hand is in the goal location. \n",
    " - The goal of the agent is to maintain its position at the target location for as many time steps as possible.\n",
    " - The environment solved in this project is a variant of the Reacher Environment agent by Unity.\n",
    " - The observation space consists of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm. Each action is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector should be a number between -1 and 1.\n",
    "\n",
    "## Environment Set Up\n",
    "\n",
    "Note:: We assumes you have [NumPy](http://www.numpy.org/), [Matplotlib](https://matplotlib.org/), [Pytorch](https://pytorch.org/) and [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) installed. Please read the README.md file for instructions on how to install these prerequisites. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This imports the necessary packages. \n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "from ddpg_agent import MultiAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the environment\n",
    "\n",
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, for the headless Linux version, the code will look like:\n",
    "\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher_Linux_NoVis/Reacher.x86_64\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "#Loading the environment. To use please replace the value of the file_name with the location of the reacher environment.\n",
    "env = UnityEnvironment(file_name='Reacher.app')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unity environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "The following code cell prints out some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Train the MultiAgent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment to train mode\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# Create agent\n",
    "agent = MultiAgent(state_size=state_size, action_size=action_size, n_agents=num_agents, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN SETTINGS\n",
    "max_t = 1000        # max number of timesteps per episode\n",
    "solved_score = 30.0 # Score that must be met to be considered solved\n",
    "solved_window = 100 # number of episodes used to calculate mean to check if solution satisified\n",
    "print_every = 10    # How often to keep a persistent version of printout\n",
    "\n",
    "# HISTORY - keep track of progress\n",
    "scores_deque = deque(maxlen=solved_window) # used to average over window of last scores\n",
    "scores = []         # track all history of episode scores\n",
    "best_average_score = -np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code that will actually loop through many rounds of training an agent to interact with the environment. The loop stops once the agent has solved the problem. A snapshot of the agent is saved for each episode in which there is an improvement in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING\n",
      "============================================================\n",
      "Using Device:  cpu\n",
      "Episode:     1\tTime (Mins): 0.55\tAverage Score:  0.17\tCurrent Score:  0.17"
     ]
    }
   ],
   "source": [
    "# TRAIN LOOP\n",
    "\n",
    "epochs = 1000 # number of training epochs\n",
    "print(\"{sep}\\nTRAINING\\n{sep}\".format(sep=\"=\"*60))\n",
    "print(\"Using Device: \", agent.device)\n",
    "t0 =  time.time()\n",
    "for i_episode in range(1, epochs+1):\n",
    "    # RESET ENVIRONMENT, STATE AND SCORES\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "    episode_scores = np.zeros(num_agents) # score for each agent within episode\n",
    "    agent.reset()\n",
    "\n",
    "    # COLLECT AN EPISODE OF EXPERIENCE FROM EACH AGENT\n",
    "    for t in range(max_t):\n",
    "        # A single step of interaction with the environment for each agent\n",
    "        actions = agent.act(states)\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "\n",
    "        # Perform step of caching experience / training on experiences\n",
    "        agent.step(states=states, actions=actions, rewards=rewards, next_states=next_states, dones=dones)\n",
    "\n",
    "        # Sum up rewards separately for each agent\n",
    "        episode_scores += np.array(rewards)\n",
    "\n",
    "        # Prepare for next timestep of iteraction\n",
    "        states = next_states  # new states become the current states\n",
    "\n",
    "        # Check if any of the agents has finished. Finish to keep all\n",
    "        # trajectories in this batch the same size.\n",
    "        if np.any(dones):\n",
    "            break\n",
    "\n",
    "    # UPDATE SCORES\n",
    "    episode_score = np.mean(episode_scores) # Summary of scores for this episode\n",
    "    scores_deque.append(episode_score)\n",
    "    average_score = np.mean(scores_deque)\n",
    "    scores.append(episode_score)\n",
    "\n",
    "    # FEEDBACK\n",
    "    t1 = (time.time()-t0)/60. # time taken so far\n",
    "    print('\\rEpisode: {: 5d}\\tTime (Mins): {:3.2f}\\tAverage Score: {: 3.2f}\\tCurrent Score: {: 3.2f}'.format(i_episode, t1, average_score, episode_score), end=\"\")\n",
    "    if i_episode % print_every == 0:\n",
    "        print('\\rEpisode: {: 5d}\\tTime (Mins): {:3.2f}\\tAverage Score: {: 3.2f}\\tCurrent Score: {: 3.2f}'.format(i_episode, t1, average_score, episode_score))\n",
    "\n",
    "    # SAVE SNAPSHOT - if it is better than previous models\n",
    "    if (i_episode >= solved_window) and (average_score > best_average_score):\n",
    "        best_average_score = average_score\n",
    "        torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "        torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "\n",
    "    # CHECK IF SOLVED\n",
    "    if average_score >= solved_score:\n",
    "        print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-solved_window, average_score))\n",
    "        torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "        torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLOSE ENVIRONMENT\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##############################################################################\n",
    "# LEARNING CURVE\n",
    "# ##############################################################################\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "fig.suptitle(\"Learning Curve\", fontsize=15)\n",
    "ax.plot(scores, color=\"#307EC7\", label=\"line\")\n",
    "ax.set_xlabel(\"Timesteps\")\n",
    "ax.set_ylabel(\"Average Score\")\n",
    "# GRID\n",
    "ax.grid(True)\n",
    "ax.grid(b=True, which='major', color='#999999', linestyle='-', linewidth=1)\n",
    "ax.minorticks_on()\n",
    "ax.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.7, linewidth=0.5)\n",
    "fig.savefig(\"learning_curves.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas for future work\n",
    "\n",
    "- We could have used different Optimization algorithm to check the difference in the Agent trained and its performance.\n",
    "- There could be better results by making use of prioritized experience replay with the existing learning algorithm.\n",
    "- Distributed Distributional Deterministic Policy Gradients (D4PG) has achieved state of the art results on continuous control problems. Also, PPO, A3C can be used in multi agent training environment.\n",
    "- It would be interesting to see how the agent performs on this environment in future implementations.\n",
    "- In future implementations,  I can try testing the agent with difference hyperparameter values, like different Sigma values, faster and smaller learning rates, tweaking the neural network, to choose the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
